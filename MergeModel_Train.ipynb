{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from utils.file_utils import *\n",
    "from datasets import list_datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from IPython import get_ipython\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from MergeModel import *\n",
    "from ClassifierModel import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bart tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50265"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/tmp/xdg-cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = dataset['train']['text']\n",
    "train_labels = torch.tensor(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = dataset['test']['text']\n",
    "val_labels = torch.tensor(dataset['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "val_batch = val_batch[:10000]\n",
    "val_labels = torch.tensor(val_labels[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 150\n",
    "input_encoding = tokenizer(train_batch, return_tensors='pt', padding=True, truncation = True, max_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encoding = tokenizer(val_batch, return_tensors='pt', padding=True, truncation = True, max_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_encoding['input_ids']\n",
    "input_mask = input_encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ids = val_encoding['input_ids']\n",
    "val_mask = val_encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape, input_mask.shape, val_ids.shape, val_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataLoaders\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# TRAINNG DATALOADER\n",
    "batch_size = 27\n",
    "\n",
    "train_data = TensorDataset(input_ids, input_mask, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader_clf = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION DATALOADER\n",
    "\n",
    "val_data = TensorDataset(val_ids, val_mask, val_labels)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader_clf = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(train_dataloader_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0].shape, b[1].shape, b[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BART Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../BART/test.csv\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = df.article.values\n",
    "train_target = df.highlights.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_points = 1000\n",
    "train_sentence = list(train_sentence[:num_data_points])\n",
    "train_target = list(train_target[:num_data_points])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_encoding = tokenizer(train_sentence, return_tensors='pt', padding=True, truncation = True, max_length=500)\n",
    "summary_encoding = tokenizer(train_target, return_tensors='pt', padding=True,truncation = True, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_input_ids = article_encoding['input_ids']\n",
    "article_attention_mask = article_encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_input_ids = summary_encoding['input_ids']\n",
    "summary_attention_mask = summary_encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_input_ids.shape, article_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_input_ids.shape, summary_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_data = TensorDataset(article_input_ids, article_attention_mask,\\\n",
    "                           summary_input_ids, summary_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(lm_logits, labels):\n",
    "    loss_fct = CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "    loss = loss_fct(lm_logits.view(-1, vocab_size), labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model = ClassifierModel(vocab_size, 64, 2, 2, 512).cuda()\n",
    "\n",
    "sent_model.load_state_dict(torch.load('classifier_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for params in sent_model.parameters():\n",
    "#     params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model = summary_model.cuda()\n",
    "senti_model = sent_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 64\n",
    "# out_dim = 2\n",
    "# n_layers = 4\n",
    "# hidden_size = 512\n",
    "merge_model = MergeModel(summary_model, senti_model)\n",
    "model_name = 'BART_classifier'\n",
    "model_dir = './experiment'\n",
    "model_path = os.path.join(model_dir,model_name)\n",
    "epochs  = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "#     'embedding_dim': embedding_dim,\n",
    "#     'out_dim': out_dim,\n",
    "#     'n_layers': n_layers,\n",
    "#     'hidden_size': hidden_size,\n",
    "    'model_name': model_name,\n",
    "    'epochs':epochs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the weighting lambdas in the main function this is just a loss without lambda weights\n",
    "def kl_div_loss(p_pred, p_target):\n",
    "    \n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    kl_div = torch.nn.KLDivLoss()\n",
    "    \n",
    "    return kl_div(logsoftmax(p_pred),softmax(p_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, train_dataloader_clf, val_dataloader=None, val_dataloader_clf = None, epochs=4, evaluation=False, model_dir='/experiment',optimizer='ADAM',config = None):\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    min_val_loss = np.inf\n",
    "    \n",
    "    model_name = config['model_name']\n",
    "    print(f\"Start training for Model {model_name}...\\n\")\n",
    "    \n",
    "    \n",
    "#     if not os.path.exists(os.path.join(model_dir,model_name)):\n",
    "#         os.mkdir(os.path.join(model_dir,model_name))\n",
    "#     model_path = os.path.join(model_dir,model_name)\n",
    "#     print(model_path)\n",
    "#     write_to_file_in_dir(model_path, 'config.json', config)\n",
    "    \n",
    "#     train_log =  'train_log.txt'\n",
    "#     write_string_train = f\"Epoch, Train_Loss, Train_Acc\"\n",
    "#     log_to_file_in_dir(model_path, train_log, write_string_train)\n",
    "\n",
    "#     if evaluation:\n",
    "#         val_log = 'val_log.txt'\n",
    "#         write_string_val = f\"Epoch, Val_Loss, Val_Acc\"\n",
    "#         log_to_file_in_dir(model_path, val_log, write_string_val)\n",
    "    \n",
    "    if optimizer == 'ADAM':\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    else:    \n",
    "        optimizer = torch.optim.SGD(model.parameters(),1e-5,momentum=0.9,weight_decay=0.01)\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Loss 1':^12} | {'Loss 2':^12} | {'Loss 3':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        total_loss, batch_loss, batch_loss1, batch_loss2, batch_loss3, batch_counts = 0, 0, 0, 0, 0, 0\n",
    "        model.train()\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            batch_counts +=1\n",
    "            \n",
    "            batch[0] = batch[0].cuda()\n",
    "            batch[1] = batch[1].cuda()\n",
    "            batch[2] = batch[2].cuda()\n",
    "            batch[3] = batch[3].cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            summary_out,*sentiments = model(batch[0],batch[1], batch[2], batch[3])\n",
    "            \n",
    "#             print(batch[2].shape, summary_out.logits.argmax(-1).shape)\n",
    "            \n",
    "            loss1 = loss_fn(summary_out.logits, batch[2])\n",
    "            \n",
    "#             loss2 = mse_loss(softmax(sentiments[0]), softmax(sentiments[1]))\n",
    "#             loss2 = mse_loss(sentiments[0], sentiments[1])\n",
    "            \n",
    "            loss2 = kl_div_loss(sentiments[0], sentiments[1])\n",
    "        \n",
    "            # Classifier\n",
    "            batch_clf = next(iter(train_dataloader_clf))\n",
    "            \n",
    "            batch_clf[0] = batch_clf[0].cuda()\n",
    "            \n",
    "            batch_clf[1] = batch_clf[1].cuda()\n",
    "            \n",
    "            batch_clf[2] = batch_clf[2].cuda()\n",
    "            \n",
    "            clf_logits = model.sentiment_model(batch_clf[0])\n",
    "            loss3 = cross_entropy(clf_logits, batch_clf[2])\n",
    "        \n",
    "            cost2, cost3 = 1e-4, 1\n",
    "            \n",
    "#             print(softmax(sentiments[1]))\n",
    "            \n",
    "#             print(softmax(sentiments[0]), softmax(sentiments[1]))\n",
    "\n",
    "            loss = loss1 + cost2 * loss2 + cost3 * loss3\n",
    "            batch_loss += loss.item()\n",
    "            batch_loss1 += loss1.item()\n",
    "            batch_loss2 += loss2.item()\n",
    "            batch_loss3 += loss3.item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "#             print(torch.autograd.grad(loss2, merge_model.summary_model.parameters(),retain_graph = True)[0])\n",
    "\n",
    "#             write_string_train = f\"{epoch_i}, {loss.item()}\"\n",
    "#             log_to_file_in_dir(model_path, train_log, write_string_train)\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step % 100 == 0) and (step != 0):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {batch_loss1 / batch_counts:^12.6f} | {batch_loss2 / batch_counts:^12.6f} | {batch_loss3 / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                batch_loss, batch_loss1, batch_loss2, batch_loss3, batch_counts = 0, 0, 0, 0, 0\n",
    "                t0_batch = time.time()\n",
    "                \n",
    "#                 print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_out.logits.argmax(dim = -1)])\n",
    "                print(\"-\"*70)\n",
    "\n",
    "        if ((epoch_i %20 ==0) and (epoch_i != 0)) or (epoch_i==epochs-1):\n",
    "            if evaluation == True:\n",
    "                val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "                val_losses.append(val_loss)\n",
    "                val_accs.append(val_accuracy)\n",
    "                \n",
    "                \n",
    "#                 write_string_val = f\"{epoch_i}, {val_loss}, {val_accuracy}\"\n",
    "#                 log_to_file_in_dir(model_path, val_log, write_string_val)\n",
    "                \n",
    "                time_elapsed = time.time() - t0_epoch\n",
    "                \n",
    "                if val_loss < min_val_loss:\n",
    "                    min_val_loss = val_loss\n",
    "                    \n",
    "                    torch.save(model, os.path.join(model_path, f'{model_name}.pt'))\n",
    "                    \n",
    "                print(f\"{epoch_i + 1:^7} | {'-':^7} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "        \n",
    "    torch.save(model.state_dict(), 'BART_classifier_final.pt')\n",
    "        \n",
    "    return  train_losses, train_accs, val_losses,val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = train(merge_model, train_dataloader, train_dataloader_clf, val_dataloader=None, val_dataloader_clf = None, epochs=epochs, evaluation=False,  config=config, optimizer='SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs, val_losses,val_accs = stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_losses)), np.array(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b1,b2,b3,b4 = next(iter(train_dataloader))\n",
    "b1 = b1.cuda()\n",
    "b2 = b2.cuda()\n",
    "b3 = b3.cuda()\n",
    "b4 = b4.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b1.shape, b2.shape, b3.shape, b4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z1 = torch.nn.Parameter(torch.ones_like(b1).float()).long()\n",
    "z2 = torch.ones_like(b2)\n",
    "\n",
    "z1_dash = z1 + 5\n",
    "z2_dash = z2 + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = summary_model(b1, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_dash = merge_model(b1, b2, b3,b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_dash[0].logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss = mse_loss(t_dash[1], t_dash[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(torch.autograd.grad(loss,merge_model.summary_model.parameters(), retain_graph=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
