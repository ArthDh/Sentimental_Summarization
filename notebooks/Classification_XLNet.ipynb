{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 8,551\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>His girlfriend bought this computer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3890</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John kept him behind the garage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2203</th>\n",
       "      <td>l-93</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>I shaved myself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7985</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The constant reading of Shakespeare satisfied me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5702</th>\n",
       "      <td>c_13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The big boy was kissed by the drooling dog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John tried to make Sam let George ask Bill to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>m_02</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>George both built the houses.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>r-67</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I gave Jack a picture of myself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>m_02</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Only to the best students would he give this b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3632</th>\n",
       "      <td>ks08</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>John might go home, so might Bill.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_source  label label_notes  \\\n",
       "3749            ks08      1         NaN   \n",
       "3890            ks08      1         NaN   \n",
       "2203            l-93      0           *   \n",
       "7985            ad03      1         NaN   \n",
       "5702            c_13      1         NaN   \n",
       "4092            ks08      1         NaN   \n",
       "6816            m_02      0           *   \n",
       "1982            r-67      1         NaN   \n",
       "6851            m_02      1         NaN   \n",
       "3632            ks08      1         NaN   \n",
       "\n",
       "                                               sentence  \n",
       "3749               His girlfriend bought this computer.  \n",
       "3890                   John kept him behind the garage.  \n",
       "2203                                   I shaved myself.  \n",
       "7985   The constant reading of Shakespeare satisfied me  \n",
       "5702        The big boy was kissed by the drooling dog.  \n",
       "4092  John tried to make Sam let George ask Bill to ...  \n",
       "6816                      George both built the houses.  \n",
       "1982                   I gave Jack a picture of myself.  \n",
       "6851  Only to the best students would he give this b...  \n",
       "3632                 John might go home, so might Bill.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "train_batch = df.sentence.values\n",
    "train_labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = list(train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8551])\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for training the XLnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased').cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy model for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# model_exp =  copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"XLNet.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for v, g in zip(model.parameters(), model_exp.parameters()):\n",
    "#     print(torch.sum(v.data - g.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Family of Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),1e-5,momentum=0.9,weight_decay=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_decay = ['bias', 'LayerNorm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# ]\n",
    "# optimizer = torch.optim.SGD(optimizer_grouped_parameters,5e-4,momentum=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AdamW\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_decay = ['bias', 'LayerNorm.weight']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# ]\n",
    "# optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XLNet Tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f500114e09b45aaaa33284f7cb4bebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer\n",
    "# Load the XLNet tokenizer.\n",
    "print('Loading XLNet Tokenizer...')\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(train_batch, return_tensors='pt', padding=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "batch_size = 16\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(input_ids, attention_mask, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for eps in range(epochs):\n",
    "    # For each batch of training data...\n",
    "    epoch_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch[0].cuda(), attention_mask=batch[1].cuda(), labels=batch[2].cuda(), return_dict=True)\n",
    "        loss = out.loss\n",
    "        epoch_loss = epoch_loss + loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "    # For each batch of training data... print loss and the batch number\n",
    "    print(\"Epoch: %d, Loss: %.3f\" % (eps, epoch_loss/step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"XLNet.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 527\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_source</th>\n",
       "      <th>label</th>\n",
       "      <th>label_notes</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>r-67</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fluffy is sick, which not everybody knows.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>sks13</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Mary revealed himself to John.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>ks08</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>Did the child be in the school?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>r-67</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I talked to Winston about himself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>c_13</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sylvia was slapping Jeff upside the head in ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>r-67</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How sane is Peter?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>ad03</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I shaved myself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>bc01</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We elected me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>bc01</td>\n",
       "      <td>0</td>\n",
       "      <td>*</td>\n",
       "      <td>What the water did to the whole bottle was fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>d_98</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Any owl hunts mice.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence_source  label label_notes  \\\n",
       "106            r-67      1         NaN   \n",
       "458           sks13      0           *   \n",
       "283            ks08      0           *   \n",
       "101            r-67      1         NaN   \n",
       "366            c_13      1         NaN   \n",
       "87             r-67      1         NaN   \n",
       "525            ad03      1         NaN   \n",
       "67             bc01      1         NaN   \n",
       "33             bc01      0           *   \n",
       "386            d_98      1         NaN   \n",
       "\n",
       "                                              sentence  \n",
       "106         Fluffy is sick, which not everybody knows.  \n",
       "458                     Mary revealed himself to John.  \n",
       "283                    Did the child be in the school?  \n",
       "101                 I talked to Winston about himself.  \n",
       "366  Sylvia was slapping Jeff upside the head in ma...  \n",
       "87                                  How sane is Peter?  \n",
       "525                                   I shaved myself.  \n",
       "67                                      We elected me.  \n",
       "33   What the water did to the whole bottle was fil...  \n",
       "386                                Any owl hunts mice.  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"in_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lists of sentences and their labels.\n",
    "test_batch = df.sentence.values\n",
    "test_labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([527])\n"
     ]
    }
   ],
   "source": [
    "test_batch = list(test_batch)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(test_batch, return_tensors='pt', padding=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "batch_size = 16\n",
    "# Create the DataLoader for our training set.\n",
    "test_data = TensorDataset(input_ids, attention_mask, test_labels)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "total_err = 0\n",
    "\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "        \n",
    "    out = model(batch[0].cuda(), attention_mask=batch[1].cuda(), return_dict=True)\n",
    "    logits = out.logits\n",
    "    _ , out_labels = logits.softmax(dim=1).max(dim = 1)\n",
    "\n",
    "    error_score = abs((out_labels - batch[2].cuda()).sum())\n",
    "    \n",
    "    total_err = total_err + error_score.item()\n",
    "    \n",
    "    print(error_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9222011385199241 486\n"
     ]
    }
   ],
   "source": [
    "print(1 - total_err/527, 527 - total_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
