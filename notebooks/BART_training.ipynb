{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 11,490\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>Ben Affleck is apologizing after it came to li...</td>\n",
       "      <td>Ben Affleck has apologized after he demanded i...</td>\n",
       "      <td>3d9c0b6d1e1093ef2918ef6c797c7f9b7b821fb5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4075</th>\n",
       "      <td>With six games to go - five for QPR and seven ...</td>\n",
       "      <td>Leicester, Burnley and QPR hoping to avoid goi...</td>\n",
       "      <td>48c5580be9e2b8b83881b1a8fd889fa13bb418c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>The BBC has refused to hand over the emails of...</td>\n",
       "      <td>Mother-of-two died following a 10-year battle ...</td>\n",
       "      <td>4822423290fd24a1a8546e7311e4830302eb55ea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>These eerie photographs show the inside of an ...</td>\n",
       "      <td>Mysterious history of Grade II listed  asylum ...</td>\n",
       "      <td>22ac75d7c650abe94f1abbcf9309556647844136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860</th>\n",
       "      <td>A man dubbed 'New Zealand's worst ever drink d...</td>\n",
       "      <td>Man set to walk free from jail just three year...</td>\n",
       "      <td>8cff9d18d04ea583c4062ac4003b31679ca7885c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11396</th>\n",
       "      <td>Jonathan Davies has warned Saracens that Clerm...</td>\n",
       "      <td>Clermont Auvergne take on Saracens in Champion...</td>\n",
       "      <td>fd62146fe8c3cf7eb37c6266ea0c81578148dd15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5251</th>\n",
       "      <td>Andres Iniesta has responded to his critics by...</td>\n",
       "      <td>Midfielder fires a warning to his detractors t...</td>\n",
       "      <td>6435a2011b44e57382d9baa8b1f98a2b3e377d7a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8108</th>\n",
       "      <td>A young fisherman in Thailand reeled in the ca...</td>\n",
       "      <td>The young boy holds a tiny  blue and yellow to...</td>\n",
       "      <td>ac4c516da1ddbf17dd3e57039bbe83d5cf3a09c8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7563</th>\n",
       "      <td>Shops offering everything for £1 or less have ...</td>\n",
       "      <td>Number of Britain's highest earners going to p...</td>\n",
       "      <td>9ef8de0159d987fba67911dc6595dbcf667b76e2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>A devastating fire has caused serious damage t...</td>\n",
       "      <td>Witnesses reported massive plumes of smoke bil...</td>\n",
       "      <td>031b2b600b555f1aa0a31189babea20fea8c3316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "3602   Ben Affleck is apologizing after it came to li...   \n",
       "4075   With six games to go - five for QPR and seven ...   \n",
       "4041   The BBC has refused to hand over the emails of...   \n",
       "2513   These eerie photographs show the inside of an ...   \n",
       "6860   A man dubbed 'New Zealand's worst ever drink d...   \n",
       "11396  Jonathan Davies has warned Saracens that Clerm...   \n",
       "5251   Andres Iniesta has responded to his critics by...   \n",
       "8108   A young fisherman in Thailand reeled in the ca...   \n",
       "7563   Shops offering everything for £1 or less have ...   \n",
       "1215   A devastating fire has caused serious damage t...   \n",
       "\n",
       "                                              highlights  \\\n",
       "3602   Ben Affleck has apologized after he demanded i...   \n",
       "4075   Leicester, Burnley and QPR hoping to avoid goi...   \n",
       "4041   Mother-of-two died following a 10-year battle ...   \n",
       "2513   Mysterious history of Grade II listed  asylum ...   \n",
       "6860   Man set to walk free from jail just three year...   \n",
       "11396  Clermont Auvergne take on Saracens in Champion...   \n",
       "5251   Midfielder fires a warning to his detractors t...   \n",
       "8108   The young boy holds a tiny  blue and yellow to...   \n",
       "7563   Number of Britain's highest earners going to p...   \n",
       "1215   Witnesses reported massive plumes of smoke bil...   \n",
       "\n",
       "                                             id  \n",
       "3602   3d9c0b6d1e1093ef2918ef6c797c7f9b7b821fb5  \n",
       "4075   48c5580be9e2b8b83881b1a8fd889fa13bb418c3  \n",
       "4041   4822423290fd24a1a8546e7311e4830302eb55ea  \n",
       "2513   22ac75d7c650abe94f1abbcf9309556647844136  \n",
       "6860   8cff9d18d04ea583c4062ac4003b31679ca7885c  \n",
       "11396  fd62146fe8c3cf7eb37c6266ea0c81578148dd15  \n",
       "5251   6435a2011b44e57382d9baa8b1f98a2b3e377d7a  \n",
       "8108   ac4c516da1ddbf17dd3e57039bbe83d5cf3a09c8  \n",
       "7563   9ef8de0159d987fba67911dc6595dbcf667b76e2  \n",
       "1215   031b2b600b555f1aa0a31189babea20fea8c3316  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = df.article.values\n",
    "train_target = df.highlights.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11490,) (11490,)\n"
     ]
    }
   ],
   "source": [
    "print(train_sentence.shape, train_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data_points = 100\n",
    "train_sentence = train_sentence[:num_data_points]\n",
    "train_target = train_target[:num_data_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = list(train_sentence)\n",
    "train_target = list(train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BART tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "# Load the BART tokenizer.\n",
    "print('Loading BART tokenizer...')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_encoding = tokenizer(train_sentence, return_tensors='pt', padding=True, truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_input_ids = article_encoding['input_ids']\n",
    "article_attention_mask = article_encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1024]) torch.Size([100, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(article_input_ids.shape, article_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding = tokenizer(train_target, return_tensors='pt', padding=True, truncation = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_input_ids = target_encoding['input_ids']\n",
    "target_attention_mask = target_encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 72]) torch.Size([100, 72])\n"
     ]
    }
   ],
   "source": [
    "print(target_input_ids.shape, target_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "batch_size = 4\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(article_input_ids, article_attention_mask, target_input_ids, target_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function\n",
    "def loss_fn(lm_logits, labels):\n",
    "    loss_fct = CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "    loss = loss_fct(lm_logits.view(-1, vocab_size), labels.view(-1))\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the model\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "model = model.cuda()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),1e-3,momentum=0.9,weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch: 0, Loss: 86.300\n",
      "Epoch:  1\n",
      "Epoch: 1, Loss: 65.039\n",
      "Epoch:  2\n",
      "Epoch: 2, Loss: 55.634\n",
      "Epoch:  3\n",
      "Epoch: 3, Loss: 50.394\n",
      "Epoch:  4\n",
      "Epoch: 4, Loss: 46.293\n",
      "Epoch:  5\n",
      "Epoch: 5, Loss: 41.135\n",
      "Epoch:  6\n",
      "Epoch: 6, Loss: 38.265\n",
      "Epoch:  7\n",
      "Epoch: 7, Loss: 34.882\n",
      "Epoch:  8\n",
      "Epoch: 8, Loss: 32.798\n",
      "Epoch:  9\n",
      "Epoch: 9, Loss: 29.365\n",
      "Epoch:  10\n",
      "Epoch: 10, Loss: 27.291\n",
      "Epoch:  11\n",
      "Epoch: 11, Loss: 25.083\n",
      "Epoch:  12\n",
      "Epoch: 12, Loss: 23.044\n",
      "Epoch:  13\n",
      "Epoch: 13, Loss: 20.923\n",
      "Epoch:  14\n",
      "Epoch: 14, Loss: 19.129\n",
      "Epoch:  15\n",
      "Epoch: 15, Loss: 17.382\n",
      "Epoch:  16\n",
      "Epoch: 16, Loss: 16.303\n",
      "Epoch:  17\n",
      "Epoch: 17, Loss: 14.827\n",
      "Epoch:  18\n",
      "Epoch: 18, Loss: 14.197\n",
      "Epoch:  19\n",
      "Epoch: 19, Loss: 12.953\n",
      "Epoch:  20\n",
      "Epoch: 20, Loss: 11.955\n",
      "Epoch:  21\n",
      "Epoch: 21, Loss: 11.466\n",
      "Epoch:  22\n",
      "Epoch: 22, Loss: 58.834\n",
      "Epoch:  23\n",
      "Epoch: 23, Loss: 185.245\n",
      "Epoch:  24\n",
      "Epoch: 24, Loss: 170.444\n",
      "Epoch:  25\n",
      "Epoch: 25, Loss: 160.739\n",
      "Epoch:  26\n",
      "Epoch: 26, Loss: 121.642\n",
      "Epoch:  27\n",
      "Epoch: 27, Loss: 65.060\n",
      "Epoch:  28\n",
      "Epoch: 28, Loss: 35.696\n",
      "Epoch:  29\n",
      "Epoch: 29, Loss: 25.521\n",
      "Epoch:  30\n",
      "Epoch: 30, Loss: 20.797\n",
      "Epoch:  31\n",
      "Epoch: 31, Loss: 17.110\n",
      "Epoch:  32\n",
      "Epoch: 32, Loss: 15.073\n",
      "Epoch:  33\n",
      "Epoch: 33, Loss: 14.200\n",
      "Epoch:  34\n",
      "Epoch: 34, Loss: 13.438\n",
      "Epoch:  35\n",
      "Epoch: 35, Loss: 11.601\n",
      "Epoch:  36\n",
      "Epoch: 36, Loss: 11.298\n",
      "Epoch:  37\n",
      "Epoch: 37, Loss: 10.288\n",
      "Epoch:  38\n",
      "Epoch: 38, Loss: 10.262\n",
      "Epoch:  39\n",
      "Epoch: 39, Loss: 9.793\n",
      "Epoch:  40\n",
      "Epoch: 40, Loss: 9.724\n",
      "Epoch:  41\n",
      "Epoch: 41, Loss: 8.807\n",
      "Epoch:  42\n",
      "Epoch: 42, Loss: 8.621\n",
      "Epoch:  43\n",
      "Epoch: 43, Loss: 8.321\n",
      "Epoch:  44\n",
      "Epoch: 44, Loss: 7.970\n",
      "Epoch:  45\n",
      "Epoch: 45, Loss: 7.491\n",
      "Epoch:  46\n",
      "Epoch: 46, Loss: 7.608\n",
      "Epoch:  47\n",
      "Epoch: 47, Loss: 7.265\n",
      "Epoch:  48\n",
      "Epoch: 48, Loss: 7.189\n",
      "Epoch:  49\n",
      "Epoch: 49, Loss: 7.235\n",
      "Epoch:  50\n",
      "Epoch: 50, Loss: 6.738\n",
      "Epoch:  51\n",
      "Epoch: 51, Loss: 6.705\n",
      "Epoch:  52\n",
      "Epoch: 52, Loss: 6.792\n",
      "Epoch:  53\n",
      "Epoch: 53, Loss: 6.538\n",
      "Epoch:  54\n",
      "Epoch: 54, Loss: 6.612\n",
      "Epoch:  55\n",
      "Epoch: 55, Loss: 6.532\n",
      "Epoch:  56\n",
      "Epoch: 56, Loss: 6.706\n",
      "Epoch:  57\n",
      "Epoch: 57, Loss: 6.773\n",
      "Epoch:  58\n",
      "Epoch: 58, Loss: 6.333\n",
      "Epoch:  59\n",
      "Epoch: 59, Loss: 6.630\n",
      "Epoch:  60\n",
      "Epoch: 60, Loss: 6.612\n",
      "Epoch:  61\n",
      "Epoch: 61, Loss: 6.195\n",
      "Epoch:  62\n",
      "Epoch: 62, Loss: 6.564\n",
      "Epoch:  63\n",
      "Epoch: 63, Loss: 6.349\n",
      "Epoch:  64\n",
      "Epoch: 64, Loss: 6.352\n",
      "Epoch:  65\n",
      "Epoch: 65, Loss: 5.937\n",
      "Epoch:  66\n",
      "Epoch: 66, Loss: 6.286\n",
      "Epoch:  67\n",
      "Epoch: 67, Loss: 6.405\n",
      "Epoch:  68\n",
      "Epoch: 68, Loss: 6.423\n",
      "Epoch:  69\n",
      "Epoch: 69, Loss: 6.471\n",
      "Epoch:  70\n",
      "Epoch: 70, Loss: 6.465\n",
      "Epoch:  71\n",
      "Epoch: 71, Loss: 6.495\n",
      "Epoch:  72\n",
      "Epoch: 72, Loss: 6.270\n",
      "Epoch:  73\n",
      "Epoch: 73, Loss: 6.205\n",
      "Epoch:  74\n",
      "Epoch: 74, Loss: 6.374\n",
      "Epoch:  75\n",
      "Epoch: 75, Loss: 6.226\n",
      "Epoch:  76\n",
      "Epoch: 76, Loss: 6.492\n",
      "Epoch:  77\n",
      "Epoch: 77, Loss: 6.347\n",
      "Epoch:  78\n",
      "Epoch: 78, Loss: 6.520\n",
      "Epoch:  79\n",
      "Epoch: 79, Loss: 6.486\n",
      "Epoch:  80\n",
      "Epoch: 80, Loss: 6.723\n",
      "Epoch:  81\n",
      "Epoch: 81, Loss: 6.650\n",
      "Epoch:  82\n",
      "Epoch: 82, Loss: 7.012\n",
      "Epoch:  83\n",
      "Epoch: 83, Loss: 6.834\n",
      "Epoch:  84\n",
      "Epoch: 84, Loss: 6.595\n",
      "Epoch:  85\n",
      "Epoch: 85, Loss: 6.742\n",
      "Epoch:  86\n",
      "Epoch: 86, Loss: 6.718\n",
      "Epoch:  87\n",
      "Epoch: 87, Loss: 6.870\n",
      "Epoch:  88\n",
      "Epoch: 88, Loss: 7.095\n",
      "Epoch:  89\n",
      "Epoch: 89, Loss: 7.048\n",
      "Epoch:  90\n",
      "Epoch: 90, Loss: 7.010\n",
      "Epoch:  91\n",
      "Epoch: 91, Loss: 7.402\n",
      "Epoch:  92\n",
      "Epoch: 92, Loss: 7.503\n",
      "Epoch:  93\n",
      "Epoch: 93, Loss: 7.337\n",
      "Epoch:  94\n",
      "Epoch: 94, Loss: 7.214\n",
      "Epoch:  95\n",
      "Epoch: 95, Loss: 6.984\n",
      "Epoch:  96\n",
      "Epoch: 96, Loss: 7.507\n",
      "Epoch:  97\n",
      "Epoch: 97, Loss: 7.385\n",
      "Epoch:  98\n",
      "Epoch: 98, Loss: 7.449\n",
      "Epoch:  99\n",
      "Epoch: 99, Loss: 7.630\n",
      "Epoch:  100\n",
      "Epoch: 100, Loss: 7.586\n",
      "Epoch:  101\n",
      "Epoch: 101, Loss: 7.947\n",
      "Epoch:  102\n",
      "Epoch: 102, Loss: 7.968\n",
      "Epoch:  103\n",
      "Epoch: 103, Loss: 7.990\n",
      "Epoch:  104\n",
      "Epoch: 104, Loss: 7.888\n",
      "Epoch:  105\n",
      "Epoch: 105, Loss: 7.843\n",
      "Epoch:  106\n",
      "Epoch: 106, Loss: 8.168\n",
      "Epoch:  107\n",
      "Epoch: 107, Loss: 7.906\n",
      "Epoch:  108\n",
      "Epoch: 108, Loss: 8.181\n",
      "Epoch:  109\n",
      "Epoch: 109, Loss: 8.178\n",
      "Epoch:  110\n",
      "Epoch: 110, Loss: 8.152\n",
      "Epoch:  111\n",
      "Epoch: 111, Loss: 8.094\n",
      "Epoch:  112\n",
      "Epoch: 112, Loss: 8.400\n",
      "Epoch:  113\n",
      "Epoch: 113, Loss: 8.217\n",
      "Epoch:  114\n",
      "Epoch: 114, Loss: 8.476\n",
      "Epoch:  115\n",
      "Epoch: 115, Loss: 8.256\n",
      "Epoch:  116\n",
      "Epoch: 116, Loss: 8.116\n",
      "Epoch:  117\n",
      "Epoch: 117, Loss: 8.611\n",
      "Epoch:  118\n",
      "Epoch: 118, Loss: 8.807\n",
      "Epoch:  119\n",
      "Epoch: 119, Loss: 9.141\n",
      "Epoch:  120\n",
      "Epoch: 120, Loss: 8.921\n",
      "Epoch:  121\n",
      "Epoch: 121, Loss: 9.283\n",
      "Epoch:  122\n",
      "Epoch: 122, Loss: 8.980\n",
      "Epoch:  123\n",
      "Epoch: 123, Loss: 8.971\n",
      "Epoch:  124\n",
      "Epoch: 124, Loss: 9.224\n",
      "Epoch:  125\n",
      "Epoch: 125, Loss: 9.140\n",
      "Epoch:  126\n",
      "Epoch: 126, Loss: 9.553\n",
      "Epoch:  127\n",
      "Epoch: 127, Loss: 9.264\n",
      "Epoch:  128\n",
      "Epoch: 128, Loss: 9.499\n",
      "Epoch:  129\n",
      "Epoch: 129, Loss: 9.739\n",
      "Epoch:  130\n",
      "Epoch: 130, Loss: 9.829\n",
      "Epoch:  131\n",
      "Epoch: 131, Loss: 9.495\n",
      "Epoch:  132\n",
      "Epoch: 132, Loss: 9.921\n",
      "Epoch:  133\n",
      "Epoch: 133, Loss: 9.722\n",
      "Epoch:  134\n",
      "Epoch: 134, Loss: 9.820\n",
      "Epoch:  135\n",
      "Epoch: 135, Loss: 9.940\n",
      "Epoch:  136\n",
      "Epoch: 136, Loss: 9.965\n",
      "Epoch:  137\n",
      "Epoch: 137, Loss: 10.021\n",
      "Epoch:  138\n",
      "Epoch: 138, Loss: 10.503\n",
      "Epoch:  139\n",
      "Epoch: 139, Loss: 10.150\n",
      "Epoch:  140\n",
      "Epoch: 140, Loss: 10.316\n",
      "Epoch:  141\n",
      "Epoch: 141, Loss: 10.232\n",
      "Epoch:  142\n",
      "Epoch: 142, Loss: 10.368\n",
      "Epoch:  143\n",
      "Epoch: 143, Loss: 10.429\n",
      "Epoch:  144\n",
      "Epoch: 144, Loss: 10.343\n",
      "Epoch:  145\n",
      "Epoch: 145, Loss: 10.339\n",
      "Epoch:  146\n",
      "Epoch: 146, Loss: 10.556\n",
      "Epoch:  147\n",
      "Epoch: 147, Loss: 11.515\n",
      "Epoch:  148\n",
      "Epoch: 148, Loss: 11.198\n",
      "Epoch:  149\n",
      "Epoch: 149, Loss: 11.147\n",
      "Epoch:  150\n",
      "Epoch: 150, Loss: 11.012\n",
      "Epoch:  151\n",
      "Epoch: 151, Loss: 10.945\n",
      "Epoch:  152\n",
      "Epoch: 152, Loss: 10.573\n",
      "Epoch:  153\n",
      "Epoch: 153, Loss: 10.960\n",
      "Epoch:  154\n",
      "Epoch: 154, Loss: 10.947\n",
      "Epoch:  155\n",
      "Epoch: 155, Loss: 10.800\n",
      "Epoch:  156\n",
      "Epoch: 156, Loss: 11.367\n",
      "Epoch:  157\n",
      "Epoch: 157, Loss: 11.222\n",
      "Epoch:  158\n",
      "Epoch: 158, Loss: 11.457\n",
      "Epoch:  159\n",
      "Epoch: 159, Loss: 11.976\n",
      "Epoch:  160\n",
      "Epoch: 160, Loss: 11.827\n",
      "Epoch:  161\n",
      "Epoch: 161, Loss: 11.802\n",
      "Epoch:  162\n",
      "Epoch: 162, Loss: 11.082\n",
      "Epoch:  163\n",
      "Epoch: 163, Loss: 11.362\n",
      "Epoch:  164\n",
      "Epoch: 164, Loss: 11.742\n",
      "Epoch:  165\n",
      "Epoch: 165, Loss: 11.328\n",
      "Epoch:  166\n",
      "Epoch: 166, Loss: 11.735\n",
      "Epoch:  167\n",
      "Epoch: 167, Loss: 12.026\n",
      "Epoch:  168\n",
      "Epoch: 168, Loss: 12.404\n",
      "Epoch:  169\n",
      "Epoch: 169, Loss: 12.220\n",
      "Epoch:  170\n",
      "Epoch: 170, Loss: 12.474\n",
      "Epoch:  171\n",
      "Epoch: 171, Loss: 12.102\n",
      "Epoch:  172\n",
      "Epoch: 172, Loss: 12.038\n",
      "Epoch:  173\n",
      "Epoch: 173, Loss: 12.292\n",
      "Epoch:  174\n",
      "Epoch: 174, Loss: 12.426\n",
      "Epoch:  175\n",
      "Epoch: 175, Loss: 12.408\n",
      "Epoch:  176\n",
      "Epoch: 176, Loss: 12.161\n",
      "Epoch:  177\n",
      "Epoch: 177, Loss: 12.366\n",
      "Epoch:  178\n",
      "Epoch: 178, Loss: 12.661\n",
      "Epoch:  179\n",
      "Epoch: 179, Loss: 12.784\n",
      "Epoch:  180\n",
      "Epoch: 180, Loss: 12.516\n",
      "Epoch:  181\n",
      "Epoch: 181, Loss: 12.834\n",
      "Epoch:  182\n",
      "Epoch: 182, Loss: 13.206\n",
      "Epoch:  183\n",
      "Epoch: 183, Loss: 12.966\n",
      "Epoch:  184\n",
      "Epoch: 184, Loss: 12.815\n",
      "Epoch:  185\n",
      "Epoch: 185, Loss: 12.599\n",
      "Epoch:  186\n",
      "Epoch: 186, Loss: 12.988\n",
      "Epoch:  187\n",
      "Epoch: 187, Loss: 12.830\n",
      "Epoch:  188\n",
      "Epoch: 188, Loss: 12.802\n",
      "Epoch:  189\n",
      "Epoch: 189, Loss: 12.894\n",
      "Epoch:  190\n",
      "Epoch: 190, Loss: 12.832\n",
      "Epoch:  191\n",
      "Epoch: 191, Loss: 12.572\n",
      "Epoch:  192\n",
      "Epoch: 192, Loss: 12.967\n",
      "Epoch:  193\n",
      "Epoch: 193, Loss: 12.981\n",
      "Epoch:  194\n",
      "Epoch: 194, Loss: 12.911\n",
      "Epoch:  195\n",
      "Epoch: 195, Loss: 13.040\n",
      "Epoch:  196\n",
      "Epoch: 196, Loss: 12.730\n",
      "Epoch:  197\n",
      "Epoch: 197, Loss: 12.961\n",
      "Epoch:  198\n",
      "Epoch: 198, Loss: 13.611\n",
      "Epoch:  199\n",
      "Epoch: 199, Loss: 13.745\n",
      "Epoch:  200\n",
      "Epoch: 200, Loss: 13.505\n",
      "Epoch:  201\n",
      "Epoch: 201, Loss: 13.015\n",
      "Epoch:  202\n",
      "Epoch: 202, Loss: 13.180\n",
      "Epoch:  203\n",
      "Epoch: 203, Loss: 13.629\n",
      "Epoch:  204\n",
      "Epoch: 204, Loss: 13.126\n",
      "Epoch:  205\n",
      "Epoch: 205, Loss: 13.729\n",
      "Epoch:  206\n",
      "Epoch: 206, Loss: 13.717\n",
      "Epoch:  207\n",
      "Epoch: 207, Loss: 13.680\n",
      "Epoch:  208\n",
      "Epoch: 208, Loss: 13.924\n",
      "Epoch:  209\n",
      "Epoch: 209, Loss: 13.883\n",
      "Epoch:  210\n",
      "Epoch: 210, Loss: 14.272\n",
      "Epoch:  211\n",
      "Epoch: 211, Loss: 13.813\n",
      "Epoch:  212\n",
      "Epoch: 212, Loss: 13.850\n",
      "Epoch:  213\n",
      "Epoch: 213, Loss: 13.752\n",
      "Epoch:  214\n",
      "Epoch: 214, Loss: 13.259\n",
      "Epoch:  215\n",
      "Epoch: 215, Loss: 13.488\n",
      "Epoch:  216\n",
      "Epoch: 216, Loss: 13.625\n",
      "Epoch:  217\n",
      "Epoch: 217, Loss: 13.711\n",
      "Epoch:  218\n",
      "Epoch: 218, Loss: 14.012\n",
      "Epoch:  219\n",
      "Epoch: 219, Loss: 14.273\n",
      "Epoch:  220\n",
      "Epoch: 220, Loss: 13.846\n",
      "Epoch:  221\n",
      "Epoch: 221, Loss: 13.904\n",
      "Epoch:  222\n",
      "Epoch: 222, Loss: 14.021\n",
      "Epoch:  223\n",
      "Epoch: 223, Loss: 14.208\n",
      "Epoch:  224\n",
      "Epoch: 224, Loss: 14.158\n",
      "Epoch:  225\n",
      "Epoch: 225, Loss: 13.996\n",
      "Epoch:  226\n",
      "Epoch: 226, Loss: 14.103\n",
      "Epoch:  227\n",
      "Epoch: 227, Loss: 14.364\n",
      "Epoch:  228\n",
      "Epoch: 228, Loss: 14.103\n",
      "Epoch:  229\n",
      "Epoch: 229, Loss: 14.000\n",
      "Epoch:  230\n",
      "Epoch: 230, Loss: 14.352\n",
      "Epoch:  231\n",
      "Epoch: 231, Loss: 14.237\n",
      "Epoch:  232\n",
      "Epoch: 232, Loss: 14.479\n",
      "Epoch:  233\n",
      "Epoch: 233, Loss: 14.658\n",
      "Epoch:  234\n",
      "Epoch: 234, Loss: 14.354\n",
      "Epoch:  235\n",
      "Epoch: 235, Loss: 14.639\n",
      "Epoch:  236\n",
      "Epoch: 236, Loss: 14.238\n",
      "Epoch:  237\n",
      "Epoch: 237, Loss: 14.200\n",
      "Epoch:  238\n",
      "Epoch: 238, Loss: 14.259\n",
      "Epoch:  239\n",
      "Epoch: 239, Loss: 14.420\n",
      "Epoch:  240\n",
      "Epoch: 240, Loss: 14.048\n",
      "Epoch:  241\n",
      "Epoch: 241, Loss: 14.817\n",
      "Epoch:  242\n",
      "Epoch: 242, Loss: 14.381\n",
      "Epoch:  243\n",
      "Epoch: 243, Loss: 14.533\n",
      "Epoch:  244\n",
      "Epoch: 244, Loss: 14.258\n",
      "Epoch:  245\n",
      "Epoch: 245, Loss: 14.767\n",
      "Epoch:  246\n",
      "Epoch: 246, Loss: 15.124\n",
      "Epoch:  247\n",
      "Epoch: 247, Loss: 14.795\n",
      "Epoch:  248\n",
      "Epoch: 248, Loss: 15.197\n",
      "Epoch:  249\n",
      "Epoch: 249, Loss: 14.663\n",
      "Epoch:  250\n",
      "Epoch: 250, Loss: 14.720\n",
      "Epoch:  251\n",
      "Epoch: 251, Loss: 14.811\n",
      "Epoch:  252\n",
      "Epoch: 252, Loss: 14.303\n",
      "Epoch:  253\n",
      "Epoch: 253, Loss: 14.697\n",
      "Epoch:  254\n",
      "Epoch: 254, Loss: 15.038\n",
      "Epoch:  255\n",
      "Epoch: 255, Loss: 14.930\n",
      "Epoch:  256\n",
      "Epoch: 256, Loss: 14.459\n",
      "Epoch:  257\n",
      "Epoch: 257, Loss: 15.012\n",
      "Epoch:  258\n",
      "Epoch: 258, Loss: 14.646\n",
      "Epoch:  259\n",
      "Epoch: 259, Loss: 14.875\n",
      "Epoch:  260\n",
      "Epoch: 260, Loss: 14.547\n",
      "Epoch:  261\n",
      "Epoch: 261, Loss: 14.633\n",
      "Epoch:  262\n",
      "Epoch: 262, Loss: 14.467\n",
      "Epoch:  263\n",
      "Epoch: 263, Loss: 14.626\n",
      "Epoch:  264\n",
      "Epoch: 264, Loss: 14.856\n",
      "Epoch:  265\n",
      "Epoch: 265, Loss: 14.847\n",
      "Epoch:  266\n",
      "Epoch: 266, Loss: 14.858\n",
      "Epoch:  267\n",
      "Epoch: 267, Loss: 14.986\n",
      "Epoch:  268\n",
      "Epoch: 268, Loss: 15.162\n",
      "Epoch:  269\n",
      "Epoch: 269, Loss: 14.844\n",
      "Epoch:  270\n",
      "Epoch: 270, Loss: 15.058\n",
      "Epoch:  271\n",
      "Epoch: 271, Loss: 15.422\n",
      "Epoch:  272\n",
      "Epoch: 272, Loss: 14.922\n",
      "Epoch:  273\n",
      "Epoch: 273, Loss: 15.071\n",
      "Epoch:  274\n",
      "Epoch: 274, Loss: 15.078\n",
      "Epoch:  275\n",
      "Epoch: 275, Loss: 15.129\n",
      "Epoch:  276\n",
      "Epoch: 276, Loss: 15.334\n",
      "Epoch:  277\n",
      "Epoch: 277, Loss: 14.902\n",
      "Epoch:  278\n",
      "Epoch: 278, Loss: 15.104\n",
      "Epoch:  279\n",
      "Epoch: 279, Loss: 15.216\n",
      "Epoch:  280\n",
      "Epoch: 280, Loss: 15.354\n",
      "Epoch:  281\n",
      "Epoch: 281, Loss: 15.090\n",
      "Epoch:  282\n",
      "Epoch: 282, Loss: 15.113\n",
      "Epoch:  283\n",
      "Epoch: 283, Loss: 15.438\n",
      "Epoch:  284\n",
      "Epoch: 284, Loss: 15.620\n",
      "Epoch:  285\n",
      "Epoch: 285, Loss: 15.128\n",
      "Epoch:  286\n",
      "Epoch: 286, Loss: 15.556\n",
      "Epoch:  287\n",
      "Epoch: 287, Loss: 15.499\n",
      "Epoch:  288\n",
      "Epoch: 288, Loss: 15.824\n",
      "Epoch:  289\n",
      "Epoch: 289, Loss: 15.490\n",
      "Epoch:  290\n",
      "Epoch: 290, Loss: 15.317\n",
      "Epoch:  291\n",
      "Epoch: 291, Loss: 15.452\n",
      "Epoch:  292\n",
      "Epoch: 292, Loss: 15.236\n",
      "Epoch:  293\n",
      "Epoch: 293, Loss: 15.594\n",
      "Epoch:  294\n",
      "Epoch: 294, Loss: 15.733\n",
      "Epoch:  295\n",
      "Epoch: 295, Loss: 15.625\n",
      "Epoch:  296\n",
      "Epoch: 296, Loss: 15.613\n",
      "Epoch:  297\n",
      "Epoch: 297, Loss: 15.801\n",
      "Epoch:  298\n",
      "Epoch: 298, Loss: 15.043\n",
      "Epoch:  299\n",
      "Epoch: 299, Loss: 15.101\n",
      "Epoch:  300\n",
      "Epoch: 300, Loss: 15.388\n",
      "Epoch:  301\n",
      "Epoch: 301, Loss: 15.327\n",
      "Epoch:  302\n",
      "Epoch: 302, Loss: 15.240\n",
      "Epoch:  303\n",
      "Epoch: 303, Loss: 15.440\n",
      "Epoch:  304\n",
      "Epoch: 304, Loss: 15.538\n",
      "Epoch:  305\n",
      "Epoch: 305, Loss: 15.555\n",
      "Epoch:  306\n",
      "Epoch: 306, Loss: 15.515\n",
      "Epoch:  307\n",
      "Epoch: 307, Loss: 15.441\n",
      "Epoch:  308\n",
      "Epoch: 308, Loss: 15.524\n",
      "Epoch:  309\n",
      "Epoch: 309, Loss: 15.743\n",
      "Epoch:  310\n",
      "Epoch: 310, Loss: 15.336\n",
      "Epoch:  311\n",
      "Epoch: 311, Loss: 15.740\n",
      "Epoch:  312\n",
      "Epoch: 312, Loss: 15.168\n",
      "Epoch:  313\n",
      "Epoch: 313, Loss: 15.320\n",
      "Epoch:  314\n",
      "Epoch: 314, Loss: 15.721\n",
      "Epoch:  315\n",
      "Epoch: 315, Loss: 15.810\n",
      "Epoch:  316\n",
      "Epoch: 316, Loss: 15.507\n",
      "Epoch:  317\n",
      "Epoch: 317, Loss: 15.447\n",
      "Epoch:  318\n",
      "Epoch: 318, Loss: 16.072\n",
      "Epoch:  319\n",
      "Epoch: 319, Loss: 15.760\n",
      "Epoch:  320\n",
      "Epoch: 320, Loss: 15.444\n",
      "Epoch:  321\n",
      "Epoch: 321, Loss: 15.320\n",
      "Epoch:  322\n",
      "Epoch: 322, Loss: 15.909\n",
      "Epoch:  323\n",
      "Epoch: 323, Loss: 15.629\n",
      "Epoch:  324\n",
      "Epoch: 324, Loss: 15.259\n",
      "Epoch:  325\n",
      "Epoch: 325, Loss: 15.474\n",
      "Epoch:  326\n",
      "Epoch: 326, Loss: 15.445\n",
      "Epoch:  327\n",
      "Epoch: 327, Loss: 15.572\n",
      "Epoch:  328\n",
      "Epoch: 328, Loss: 15.942\n",
      "Epoch:  329\n",
      "Epoch: 329, Loss: 15.565\n",
      "Epoch:  330\n",
      "Epoch: 330, Loss: 15.806\n",
      "Epoch:  331\n",
      "Epoch: 331, Loss: 15.740\n",
      "Epoch:  332\n",
      "Epoch: 332, Loss: 15.618\n",
      "Epoch:  333\n",
      "Epoch: 333, Loss: 15.379\n",
      "Epoch:  334\n",
      "Epoch: 334, Loss: 15.254\n",
      "Epoch:  335\n",
      "Epoch: 335, Loss: 15.463\n",
      "Epoch:  336\n",
      "Epoch: 336, Loss: 15.614\n",
      "Epoch:  337\n",
      "Epoch: 337, Loss: 15.476\n",
      "Epoch:  338\n",
      "Epoch: 338, Loss: 15.431\n",
      "Epoch:  339\n",
      "Epoch: 339, Loss: 15.269\n",
      "Epoch:  340\n",
      "Epoch: 340, Loss: 15.454\n",
      "Epoch:  341\n",
      "Epoch: 341, Loss: 15.888\n",
      "Epoch:  342\n",
      "Epoch: 342, Loss: 15.517\n",
      "Epoch:  343\n",
      "Epoch: 343, Loss: 15.565\n",
      "Epoch:  344\n",
      "Epoch: 344, Loss: 15.339\n",
      "Epoch:  345\n",
      "Epoch: 345, Loss: 15.618\n",
      "Epoch:  346\n",
      "Epoch: 346, Loss: 15.673\n",
      "Epoch:  347\n",
      "Epoch: 347, Loss: 15.820\n",
      "Epoch:  348\n",
      "Epoch: 348, Loss: 15.662\n",
      "Epoch:  349\n",
      "Epoch: 349, Loss: 16.114\n",
      "Epoch:  350\n",
      "Epoch: 350, Loss: 15.693\n",
      "Epoch:  351\n",
      "Epoch: 351, Loss: 15.302\n",
      "Epoch:  352\n",
      "Epoch: 352, Loss: 15.554\n",
      "Epoch:  353\n",
      "Epoch: 353, Loss: 15.578\n",
      "Epoch:  354\n",
      "Epoch: 354, Loss: 15.709\n",
      "Epoch:  355\n",
      "Epoch: 355, Loss: 15.648\n",
      "Epoch:  356\n",
      "Epoch: 356, Loss: 15.734\n",
      "Epoch:  357\n",
      "Epoch: 357, Loss: 15.405\n",
      "Epoch:  358\n",
      "Epoch: 358, Loss: 16.346\n",
      "Epoch:  359\n",
      "Epoch: 359, Loss: 15.555\n",
      "Epoch:  360\n",
      "Epoch: 360, Loss: 15.579\n",
      "Epoch:  361\n",
      "Epoch: 361, Loss: 15.672\n",
      "Epoch:  362\n",
      "Epoch: 362, Loss: 15.644\n",
      "Epoch:  363\n",
      "Epoch: 363, Loss: 15.357\n",
      "Epoch:  364\n",
      "Epoch: 364, Loss: 15.420\n",
      "Epoch:  365\n",
      "Epoch: 365, Loss: 15.617\n",
      "Epoch:  366\n",
      "Epoch: 366, Loss: 15.420\n",
      "Epoch:  367\n",
      "Epoch: 367, Loss: 15.786\n",
      "Epoch:  368\n",
      "Epoch: 368, Loss: 15.366\n",
      "Epoch:  369\n",
      "Epoch: 369, Loss: 15.720\n",
      "Epoch:  370\n",
      "Epoch: 370, Loss: 15.908\n",
      "Epoch:  371\n",
      "Epoch: 371, Loss: 16.061\n",
      "Epoch:  372\n",
      "Epoch: 372, Loss: 15.609\n",
      "Epoch:  373\n",
      "Epoch: 373, Loss: 15.263\n",
      "Epoch:  374\n",
      "Epoch: 374, Loss: 15.280\n",
      "Epoch:  375\n",
      "Epoch: 375, Loss: 15.223\n",
      "Epoch:  376\n",
      "Epoch: 376, Loss: 15.368\n",
      "Epoch:  377\n",
      "Epoch: 377, Loss: 15.462\n",
      "Epoch:  378\n",
      "Epoch: 378, Loss: 15.064\n",
      "Epoch:  379\n",
      "Epoch: 379, Loss: 15.187\n",
      "Epoch:  380\n",
      "Epoch: 380, Loss: 14.889\n",
      "Epoch:  381\n",
      "Epoch: 381, Loss: 15.185\n",
      "Epoch:  382\n",
      "Epoch: 382, Loss: 15.307\n",
      "Epoch:  383\n",
      "Epoch: 383, Loss: 15.168\n",
      "Epoch:  384\n",
      "Epoch: 384, Loss: 15.014\n",
      "Epoch:  385\n",
      "Epoch: 385, Loss: 15.445\n",
      "Epoch:  386\n",
      "Epoch: 386, Loss: 15.357\n",
      "Epoch:  387\n",
      "Epoch: 387, Loss: 14.732\n",
      "Epoch:  388\n",
      "Epoch: 388, Loss: 15.014\n",
      "Epoch:  389\n",
      "Epoch: 389, Loss: 15.021\n",
      "Epoch:  390\n",
      "Epoch: 390, Loss: 15.086\n",
      "Epoch:  391\n",
      "Epoch: 391, Loss: 15.311\n",
      "Epoch:  392\n",
      "Epoch: 392, Loss: 15.033\n",
      "Epoch:  393\n",
      "Epoch: 393, Loss: 14.935\n",
      "Epoch:  394\n",
      "Epoch: 394, Loss: 14.805\n",
      "Epoch:  395\n",
      "Epoch: 395, Loss: 15.011\n",
      "Epoch:  396\n",
      "Epoch: 396, Loss: 15.238\n",
      "Epoch:  397\n",
      "Epoch: 397, Loss: 15.415\n",
      "Epoch:  398\n",
      "Epoch: 398, Loss: 15.261\n",
      "Epoch:  399\n",
      "Epoch: 399, Loss: 15.120\n",
      "Epoch:  400\n",
      "Epoch: 400, Loss: 15.050\n",
      "Epoch:  401\n",
      "Epoch: 401, Loss: 14.879\n",
      "Epoch:  402\n",
      "Epoch: 402, Loss: 15.147\n",
      "Epoch:  403\n",
      "Epoch: 403, Loss: 15.039\n",
      "Epoch:  404\n",
      "Epoch: 404, Loss: 14.703\n",
      "Epoch:  405\n",
      "Epoch: 405, Loss: 14.819\n",
      "Epoch:  406\n",
      "Epoch: 406, Loss: 15.112\n",
      "Epoch:  407\n",
      "Epoch: 407, Loss: 15.246\n",
      "Epoch:  408\n",
      "Epoch: 408, Loss: 14.999\n",
      "Epoch:  409\n",
      "Epoch: 409, Loss: 15.174\n",
      "Epoch:  410\n",
      "Epoch: 410, Loss: 14.739\n",
      "Epoch:  411\n",
      "Epoch: 411, Loss: 14.780\n",
      "Epoch:  412\n",
      "Epoch: 412, Loss: 15.050\n",
      "Epoch:  413\n",
      "Epoch: 413, Loss: 14.711\n",
      "Epoch:  414\n",
      "Epoch: 414, Loss: 14.769\n",
      "Epoch:  415\n",
      "Epoch: 415, Loss: 14.802\n",
      "Epoch:  416\n",
      "Epoch: 416, Loss: 14.981\n",
      "Epoch:  417\n",
      "Epoch: 417, Loss: 14.901\n",
      "Epoch:  418\n",
      "Epoch: 418, Loss: 14.293\n",
      "Epoch:  419\n",
      "Epoch: 419, Loss: 14.642\n",
      "Epoch:  420\n",
      "Epoch: 420, Loss: 14.389\n",
      "Epoch:  421\n",
      "Epoch: 421, Loss: 14.605\n",
      "Epoch:  422\n",
      "Epoch: 422, Loss: 14.299\n",
      "Epoch:  423\n",
      "Epoch: 423, Loss: 14.239\n",
      "Epoch:  424\n",
      "Epoch: 424, Loss: 14.237\n",
      "Epoch:  425\n",
      "Epoch: 425, Loss: 14.507\n",
      "Epoch:  426\n",
      "Epoch: 426, Loss: 14.637\n",
      "Epoch:  427\n",
      "Epoch: 427, Loss: 14.366\n",
      "Epoch:  428\n",
      "Epoch: 428, Loss: 14.555\n",
      "Epoch:  429\n",
      "Epoch: 429, Loss: 14.401\n",
      "Epoch:  430\n",
      "Epoch: 430, Loss: 14.437\n",
      "Epoch:  431\n",
      "Epoch: 431, Loss: 14.200\n",
      "Epoch:  432\n",
      "Epoch: 432, Loss: 14.331\n",
      "Epoch:  433\n",
      "Epoch: 433, Loss: 13.928\n",
      "Epoch:  434\n",
      "Epoch: 434, Loss: 13.964\n",
      "Epoch:  435\n",
      "Epoch: 435, Loss: 14.026\n",
      "Epoch:  436\n",
      "Epoch: 436, Loss: 14.408\n",
      "Epoch:  437\n",
      "Epoch: 437, Loss: 14.196\n",
      "Epoch:  438\n",
      "Epoch: 438, Loss: 14.014\n",
      "Epoch:  439\n",
      "Epoch: 439, Loss: 14.188\n",
      "Epoch:  440\n",
      "Epoch: 440, Loss: 14.052\n",
      "Epoch:  441\n",
      "Epoch: 441, Loss: 13.924\n",
      "Epoch:  442\n",
      "Epoch: 442, Loss: 14.073\n",
      "Epoch:  443\n",
      "Epoch: 443, Loss: 13.895\n",
      "Epoch:  444\n",
      "Epoch: 444, Loss: 13.832\n",
      "Epoch:  445\n",
      "Epoch: 445, Loss: 14.371\n",
      "Epoch:  446\n",
      "Epoch: 446, Loss: 14.197\n",
      "Epoch:  447\n",
      "Epoch: 447, Loss: 14.073\n",
      "Epoch:  448\n",
      "Epoch: 448, Loss: 13.986\n",
      "Epoch:  449\n",
      "Epoch: 449, Loss: 13.718\n",
      "Epoch:  450\n",
      "Epoch: 450, Loss: 14.024\n",
      "Epoch:  451\n",
      "Epoch: 451, Loss: 13.976\n",
      "Epoch:  452\n",
      "Epoch: 452, Loss: 13.832\n",
      "Epoch:  453\n",
      "Epoch: 453, Loss: 13.692\n",
      "Epoch:  454\n",
      "Epoch: 454, Loss: 13.809\n",
      "Epoch:  455\n",
      "Epoch: 455, Loss: 13.698\n",
      "Epoch:  456\n",
      "Epoch: 456, Loss: 13.560\n",
      "Epoch:  457\n",
      "Epoch: 457, Loss: 13.460\n",
      "Epoch:  458\n",
      "Epoch: 458, Loss: 13.907\n",
      "Epoch:  459\n",
      "Epoch: 459, Loss: 14.023\n",
      "Epoch:  460\n",
      "Epoch: 460, Loss: 13.601\n",
      "Epoch:  461\n",
      "Epoch: 461, Loss: 13.627\n",
      "Epoch:  462\n",
      "Epoch: 462, Loss: 13.847\n",
      "Epoch:  463\n",
      "Epoch: 463, Loss: 13.858\n",
      "Epoch:  464\n",
      "Epoch: 464, Loss: 13.476\n",
      "Epoch:  465\n",
      "Epoch: 465, Loss: 13.803\n",
      "Epoch:  466\n",
      "Epoch: 466, Loss: 13.693\n",
      "Epoch:  467\n",
      "Epoch: 467, Loss: 13.769\n",
      "Epoch:  468\n",
      "Epoch: 468, Loss: 13.372\n",
      "Epoch:  469\n",
      "Epoch: 469, Loss: 13.485\n",
      "Epoch:  470\n",
      "Epoch: 470, Loss: 13.581\n",
      "Epoch:  471\n",
      "Epoch: 471, Loss: 13.702\n",
      "Epoch:  472\n",
      "Epoch: 472, Loss: 13.843\n",
      "Epoch:  473\n",
      "Epoch: 473, Loss: 13.519\n",
      "Epoch:  474\n",
      "Epoch: 474, Loss: 13.457\n",
      "Epoch:  475\n",
      "Epoch: 475, Loss: 13.763\n",
      "Epoch:  476\n",
      "Epoch: 476, Loss: 13.260\n",
      "Epoch:  477\n",
      "Epoch: 477, Loss: 13.050\n",
      "Epoch:  478\n",
      "Epoch: 478, Loss: 13.447\n",
      "Epoch:  479\n",
      "Epoch: 479, Loss: 13.162\n",
      "Epoch:  480\n",
      "Epoch: 480, Loss: 12.814\n",
      "Epoch:  481\n",
      "Epoch: 481, Loss: 13.239\n",
      "Epoch:  482\n",
      "Epoch: 482, Loss: 13.176\n",
      "Epoch:  483\n",
      "Epoch: 483, Loss: 13.105\n",
      "Epoch:  484\n",
      "Epoch: 484, Loss: 13.476\n",
      "Epoch:  485\n",
      "Epoch: 485, Loss: 13.639\n",
      "Epoch:  486\n",
      "Epoch: 486, Loss: 13.325\n",
      "Epoch:  487\n",
      "Epoch: 487, Loss: 13.105\n",
      "Epoch:  488\n",
      "Epoch: 488, Loss: 13.192\n",
      "Epoch:  489\n",
      "Epoch: 489, Loss: 13.082\n",
      "Epoch:  490\n",
      "Epoch: 490, Loss: 13.031\n",
      "Epoch:  491\n",
      "Epoch: 491, Loss: 12.975\n",
      "Epoch:  492\n",
      "Epoch: 492, Loss: 12.883\n",
      "Epoch:  493\n",
      "Epoch: 493, Loss: 13.006\n",
      "Epoch:  494\n",
      "Epoch: 494, Loss: 12.617\n",
      "Epoch:  495\n",
      "Epoch: 495, Loss: 12.893\n",
      "Epoch:  496\n",
      "Epoch: 496, Loss: 12.989\n",
      "Epoch:  497\n",
      "Epoch: 497, Loss: 13.101\n",
      "Epoch:  498\n",
      "Epoch: 498, Loss: 13.021\n",
      "Epoch:  499\n",
      "Epoch: 499, Loss: 12.808\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "for eps in range(epochs):\n",
    "    # For each batch of training data...\n",
    "    print('Epoch: ', eps)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # push the batch to the cuda\n",
    "        batch[0] = batch[0].cuda()\n",
    "        batch[1] = batch[1].cuda()\n",
    "        batch[2] = batch[2].cuda()\n",
    "        batch[3] = batch[3].cuda()\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = model(input_ids=batch[0], attention_mask =batch[1], labels=batch[2], decoder_attention_mask=batch[3], return_dict=True)\n",
    "        \n",
    "        loss = loss_fn(out.logits, batch[2])\n",
    "        \n",
    "        epoch_loss = epoch_loss + loss\n",
    "        \n",
    "        # print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in out.logits.argmax(dim = 2)])\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # For each batch of training data... print loss and the batch number\n",
    "    print(\"Epoch: %d, Loss: %.3f\" % (eps, epoch_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_ids = model.generate(article_input_ids[0].view(1,-1).cuda(), num_beams=4, max_length = 72, early_stopping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James Best, who played the sheriff on \"The Dukes of Hazzard,\" died Monday at 88 .\\n\"Hazzard\" ran from 1979 to 1985 and was among the most popular shows on TV .']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['James Best, who played the sheriff on \"The Dukes of Hazzard,\" died Monday at 88 .\\n\"Hazzard\" ran from 1979 to 1985 and was among the most popular shows on TV .']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in target_input_ids[0].view(1,-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
