{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "from utils.file_utils import *\n",
    "from datasets import list_datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from IPython import get_ipython\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from MergeModel import *\n",
    "from ClassifierModel import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bart tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = dataset['train']['text']\n",
    "train_labels = torch.tensor(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = dataset['test']['text']\n",
    "val_labels = torch.tensor(dataset['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 150\n",
    "input_encoding = tokenizer(train_batch, return_tensors='pt', padding=True, truncation = True, max_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encoding = tokenizer(val_batch, return_tensors='pt', padding=True, truncation = True, max_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = input_encoding['input_ids']\n",
    "input_mask = input_encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ids = val_encoding['input_ids']\n",
    "val_mask = val_encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape, input_mask.shape, val_ids.shape, val_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataLoaders\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "# TRAINNG DATALOADER\n",
    "batch_size = 16\n",
    "\n",
    "train_data = TensorDataset(input_ids, input_mask, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader_clf = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # VALIDATION DATALOADER\n",
    "\n",
    "# val_data = TensorDataset(val_ids, val_mask, val_labels)\n",
    "# val_sampler = RandomSampler(val_data)\n",
    "# val_dataloader_clf = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = next(iter(train_dataloader_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b[0].shape, b[1].shape, b[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BART Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../BART/test.csv\")\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentence = df.article.values\n",
    "train_target = df.highlights.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_data_points = 10000\n",
    "train_sentence = list(train_sentence)\n",
    "train_target = list(train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_encoding = tokenizer(train_sentence, return_tensors='pt', padding=True, truncation = True, max_length=500)\n",
    "summary_encoding = tokenizer(train_target, return_tensors='pt', padding=True,truncation = True, max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_input_ids = article_encoding['input_ids']\n",
    "article_attention_mask = article_encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_input_ids = summary_encoding['input_ids']\n",
    "summary_attention_mask = summary_encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_input_ids.shape, article_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_input_ids.shape, summary_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_data = TensorDataset(article_input_ids, article_attention_mask,\\\n",
    "                           summary_input_ids, summary_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(lm_logits, labels):\n",
    "    loss_fct = CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
    "    loss = loss_fct(lm_logits.view(-1, vocab_size), labels.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_model = ClassifierModel(vocab_size, 64, 2, 2, 512).cuda()\n",
    "\n",
    "sent_model.load_state_dict(torch.load('classifier_model_senticlf.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_model = summary_model.cuda()\n",
    "senti_model = sent_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 64\n",
    "# out_dim = 2\n",
    "# n_layers = 4\n",
    "# hidden_size = 512\n",
    "merge_model = MergeModel(summary_model, senti_model).cuda()\n",
    "model_name = 'BART_classifier'\n",
    "model_dir = './experiment'\n",
    "model_path = os.path.join(model_dir,model_name)\n",
    "epochs  = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for params in merge_model.sentiment_model.parameters():\n",
    "#     params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "#     'embedding_dim': embedding_dim,\n",
    "#     'out_dim': out_dim,\n",
    "#     'n_layers': n_layers,\n",
    "#     'hidden_size': hidden_size,\n",
    "    'model_name': model_name,\n",
    "    'epochs':epochs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the weighting lambdas in the main function this is just a loss without lambda weights\n",
    "def kl_div_loss(p_pred, p_target):\n",
    "    \n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    kl_div = torch.nn.KLDivLoss()\n",
    "    \n",
    "    return kl_div(logsoftmax(p_pred),softmax(p_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=-1)\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False, model_dir='/experiment',config = None):\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    min_val_loss = np.inf\n",
    "    \n",
    "    model_name = config['model_name']\n",
    "    print(f\"Start training for Model {model_name}...\\n\")\n",
    "    \n",
    "    \n",
    "#     if not os.path.exists(os.path.join(model_dir,model_name)):\n",
    "#         os.mkdir(os.path.join(model_dir,model_name))\n",
    "#     model_path = os.path.join(model_dir,model_name)\n",
    "#     print(model_path)\n",
    "#     write_to_file_in_dir(model_path, 'config.json', config)\n",
    "    \n",
    "#     train_log =  'train_log.txt'\n",
    "#     write_string_train = f\"Epoch, Train_Loss, Train_Acc\"\n",
    "#     log_to_file_in_dir(model_path, train_log, write_string_train)\n",
    "\n",
    "#     if evaluation:\n",
    "#         val_log = 'val_log.txt'\n",
    "#         write_string_val = f\"Epoch, Val_Loss, Val_Acc\"\n",
    "#         log_to_file_in_dir(model_path, val_log, write_string_val)\n",
    "    \n",
    "#     from transformers import AdamW\n",
    "#     bart_optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    bart_optimizer = torch.optim.SGD(model.summary_model.parameters(),1e-5,momentum=0.9)\n",
    "    \n",
    "    sentiment_optimizer = torch.optim.Adam(model.sentiment_model.parameters())\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Loss 1':^12} | {'Loss 2':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        total_loss, batch_loss, batch_loss1, batch_loss2, batch_loss_clf, batch_counts = 0, 0, 0, 0, 0, 0\n",
    "        model.train()\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            model.train()\n",
    "            batch_counts +=1\n",
    "            \n",
    "            batch[0] = batch[0].cuda()\n",
    "            batch[1] = batch[1].cuda()\n",
    "            batch[2] = batch[2].cuda()\n",
    "            batch[3] = batch[3].cuda()\n",
    "            \n",
    "            bart_optimizer.zero_grad()\n",
    "            \n",
    "            if epoch < 10:\n",
    "                sentiment_optimizer.zero_grad()\n",
    "                # get a random minibatch from the search queue with replacement\n",
    "                batch_clf  = next(iter(train_dataloader_clf))\n",
    "                input_clf  = batch_clf[0].cuda()\n",
    "                target_clf = batch_clf[2].cuda()\n",
    "\n",
    "                logits = model.sentiment_model(input_clf)\n",
    "                loss_clf = cross_entropy(logits, target_clf)\n",
    "            else:\n",
    "                loss_clf = 0\n",
    "            \n",
    "            if epoch == 10:\n",
    "                for params in merge_model.sentiment_model.parameters():\n",
    "                    params.requires_grad = False\n",
    "            \n",
    "            summary_out,*sentiments = model(batch[0],batch[1], batch[2], batch[3])\n",
    "        \n",
    "            cost1, cost2 = 2, 1e-2\n",
    "            \n",
    "            loss1 = cost1*loss_fn(summary_out.logits, batch[2])\n",
    "            \n",
    "            loss2 = cost2*kl_div_loss(sentiments[0], sentiments[1])\n",
    "            \n",
    "            loss = loss1 + loss2 + loss_clf\n",
    "            \n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "            batch_loss1 += loss1.item()\n",
    "            \n",
    "            batch_loss2 += loss2.item()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            # print(torch.autograd.grad(loss2, merge_model.summary_model.parameters(),retain_graph = True)[0])\n",
    "\n",
    "            # write_string_train = f\"{epoch_i}, {loss.item()}\"\n",
    "            # log_to_file_in_dir(model_path, train_log, write_string_train)\n",
    "            \n",
    "            loss1.backward(retain_graph = True)\n",
    "            \n",
    "            loss2.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 5)\n",
    "            \n",
    "            bart_optimizer.step()\n",
    "            \n",
    "            if epoch < 10:\n",
    "                sentiment_optimizer.step()\n",
    "            \n",
    "            if (step % 100 == 0) and (step != 0):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {batch_loss1 / batch_counts:^12.6f} | {batch_loss2 / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                batch_loss, batch_loss1, batch_loss2, batch_counts = 0, 0, 0, 0\n",
    "                t0_batch = time.time()\n",
    "                \n",
    "                print(\"-\"*70)\n",
    "                # print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_out.logits.argmax(dim = -1)])\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        if ((epoch_i %10 ==0) and (epoch_i != 0)) or (epoch_i==epochs-1):\n",
    "            \n",
    "            print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_out.logits.argmax(dim = -1)])\n",
    "        \n",
    "            \n",
    "        \n",
    "    torch.save(model.state_dict(), 'BART_classifier_final.pt')\n",
    "        \n",
    "    return  train_losses, train_accs, val_losses,val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = train(merge_model, train_dataloader, val_dataloader=None, epochs=epochs, evaluation=False,  config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(merge_model.state_dict(), 'BART_classifier_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs, val_losses,val_accs = stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_losses)), np.array(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b1,b2,b3,b4 = next(iter(train_dataloader))\n",
    "b1 = b1.cuda()\n",
    "b2 = b2.cuda()\n",
    "b3 = b3.cuda()\n",
    "b4 = b4.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b1.shape, b2.shape, b3.shape, b4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z1 = torch.nn.Parameter(torch.ones_like(b1).float()).long()\n",
    "z2 = torch.ones_like(b2)\n",
    "\n",
    "z1_dash = z1 + 5\n",
    "z2_dash = z2 + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t = summary_model(b1, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_dash = merge_model(b1, b2, b3,b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t_dash[0].logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss = mse_loss(t_dash[1], t_dash[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(torch.autograd.grad(loss,merge_model.summary_model.parameters(), retain_graph=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
